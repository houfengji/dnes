\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{brewer11a}
\citation{goodman10a}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{eq:bayesian-decision-theory}{{1}{1}{Introduction\relax }{equation.1.1}{}}
\citation{skilling06a}
\@writefile{toc}{\contentsline {section}{\numberline {2}Diffusive Nested Sampling}{2}{section.2}}
\newlabel{eq:prior-mass}{{5}{2}{Diffusive Nested Sampling\relax }{equation.2.5}{}}
\newlabel{eq:evidence-prior-mass}{{8}{2}{Diffusive Nested Sampling\relax }{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Level and Constrained Prior}{2}{subsection.2.1}}
\newlabel{eq:constrained-prior}{{9}{2}{Level and Constrained Prior\relax }{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Setting Level Thresholds}{3}{subsection.2.2}}
\newlabel{eq:mixture-constrained-prior}{{10}{3}{Setting Level Thresholds\relax }{equation.2.10}{}}
\citation{goodman10a}
\citation{brewer11a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Affine Invariant Stretch Move}{4}{subsection.2.3}}
\citation{brewer11a}
\citation{brewer11a}
\newlabel{eq:index-proposal-prob}{{13}{5}{Affine Invariant Stretch Move\relax }{equation.2.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Refining Level Masses}{5}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Computing Evidence}{6}{subsection.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}2-d Gaussian Testing Case}{6}{section.3}}
\newlabel{eq:likelihood2}{{18}{6}{2-d Gaussian Testing Case\relax }{equation.3.18}{}}
\newlabel{eq:prior2}{{19}{6}{2-d Gaussian Testing Case\relax }{equation.3.19}{}}
\newlabel{eq:analytical-threshold}{{21}{6}{2-d Gaussian Testing Case\relax }{equation.3.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Testing Level Thresholds Setting}{6}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Testing Constrained Prior Mixture}{7}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}High-Dimension Gaussian Testing Case}{7}{section.4}}
\newlabel{eq:likelihood10}{{22}{7}{High-Dimension Gaussian Testing Case\relax }{equation.4.22}{}}
\newlabel{eq:prior10}{{23}{7}{High-Dimension Gaussian Testing Case\relax }{equation.4.23}{}}
\bibcite{goodman10a}{{1}{2010}{{Goodman \emph  {et\tmspace  +\thinmuskip {.1667em}al.}}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Testing Prior Mass Refinement}{8}{subsection.4.1}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Exoplanet for Star 122}{8}{section.5}}
\bibcite{brewer11a}{{2}{2011}{{Brewer \emph  {et\tmspace  +\thinmuskip {.1667em}al.}}}{{}}}
\bibcite{skilling06a}{{3}{2006}{{Skilling }}{{}}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The optimal-fit parameters for all 3 models are summarized here. The 1st row lists the log likelihoods of the optimal-fit parameters of these 3 models. }}{9}{table.1}}
\newlabel{tab:best-fit-star-122}{{1}{9}{The optimal-fit parameters for all 3 models are summarized here. The 1st row lists the log likelihoods of the optimal-fit parameters of these 3 models. \relax }{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The experiment was repeated $10,000$ times. The experimental mean values of the logarithm of the 6 levels' thresholds in the table are the mean of the $10,000$ repetitions. Notice that the experimental means for $N_{1b}$ tend to be closer to analytical values than those for $N_{1a}$ as expected.}}{10}{table.2}}
\newlabel{tab:threshold-mean}{{2}{10}{The experiment was repeated $10,000$ times. The experimental mean values of the logarithm of the 6 levels' thresholds in the table are the mean of the $10,000$ repetitions. Notice that the experimental means for $N_{1b}$ tend to be closer to analytical values than those for $N_{1a}$ as expected}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The experiment was repeated $10,000$ times. The experimental std of the logarithm of the 6 levels' thresholds in the table are the variance of the $10,000$ repetitions. The algorithm almost achieves the expected precision. Note that the std's for $N_{1a}$ are approximately $\sqrt  {10}$ times those for $N_{1b}$.}}{10}{table.3}}
\newlabel{tab:threshold-variance}{{3}{10}{The experiment was repeated $10,000$ times. The experimental std of the logarithm of the 6 levels' thresholds in the table are the variance of the $10,000$ repetitions. The algorithm almost achieves the expected precision. Note that the std's for $N_{1a}$ are approximately $\sqrt {10}$ times those for $N_{1b}$}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces True levels' thresholds and prior masses are listed in the table. 10 Levels are given. We keep many digits because these are true levels.}}{11}{table.4}}
\newlabel{tab:true-levels}{{4}{11}{True levels' thresholds and prior masses are listed in the table. 10 Levels are given. We keep many digits because these are true levels}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The standard deviations for different $N_1$'s and $N_2$'s. The experiments are repeated for $1,000$ times. Compared with evidence value $9.77\times 10^{-14}$, all the standard deviations are reasonably small. But $N_2$ is clearly more important in reducing variance. Note that there are 30 levels in this case, $30\times N_1$ and $N_2$ are actually comparable.}}{11}{table.5}}
\newlabel{tab:N1N2}{{5}{11}{The standard deviations for different $N_1$'s and $N_2$'s. The experiments are repeated for $1,000$ times. Compared with evidence value $9.77\times 10^{-14}$, all the standard deviations are reasonably small. But $N_2$ is clearly more important in reducing variance. Note that there are 30 levels in this case, $30\times N_1$ and $N_2$ are actually comparable}{table.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Level 1 (upper) and Level 6 (lower): Both histograms are plotted with 25 bins and 10,000 samples. (Here samples mean repetitions of the experiment, not the samples of likelihood to build every single level.) The left-hand side is the histogram of levels built with $N_{1a}$ likelihoods and the right-hand side is the histogram of levels built with $N_{1b}$ likelihoods. The \leavevmode {\color  {red}red} solid lines indicate the true values of the logarithm of the likelihood thresholds of levels and \leavevmode {\color  {darkgreen}dark green} solid lines indicate the experimental mean of the logarithm of the likelihood thresholds, which cannot be distinguished from the true values in these pictures. The \leavevmode {\color  {red}red} dashed lines indicate the theoretical standard deviation and the \leavevmode {\color  {darkgreen}dark green} dashed lines indicate the experimental standard deviation.}}{12}{figure.1}}
\newlabel{fig:level1-6}{{1}{12}{Level 1 (upper) and Level 6 (lower): Both histograms are plotted with 25 bins and 10,000 samples. (Here samples mean repetitions of the experiment, not the samples of likelihood to build every single level.) The left-hand side is the histogram of levels built with $N_{1a}$ likelihoods and the right-hand side is the histogram of levels built with $N_{1b}$ likelihoods. The \textcolor {red}{red} solid lines indicate the true values of the logarithm of the likelihood thresholds of levels and \textcolor {darkgreen}{dark green} solid lines indicate the experimental mean of the logarithm of the likelihood thresholds, which cannot be distinguished from the true values in these pictures. The \textcolor {red}{red} dashed lines indicate the theoretical standard deviation and the \textcolor {darkgreen}{dark green} dashed lines indicate the experimental standard deviation}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Histograms of prior masses of samples inside a bin sandwiched by two adjacent levels. 4 examples are given. All are approximately uniform distribution.}}{13}{figure.2}}
\newlabel{fig:hist-gaps}{{2}{13}{Histograms of prior masses of samples inside a bin sandwiched by two adjacent levels. 4 examples are given. All are approximately uniform distribution}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The 10 levels in the figure are the true levels summarized in Tab. (\ref  {tab:true-levels}). Notice that the standard deviation of likelihood samples between level 3 ($\qopname  \relax o{log}{M} = -3$) and level 6 ($\qopname  \relax o{log}{M} = -6$) will pretty much determine the standard deviation of the final result.}}{14}{figure.3}}
\newlabel{fig:level-var}{{3}{14}{The 10 levels in the figure are the true levels summarized in Tab. (\ref {tab:true-levels}). Notice that the standard deviation of likelihood samples between level 3 ($\log {M} = -3$) and level 6 ($\log {M} = -6$) will pretty much determine the standard deviation of the final result}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The 1st row shows the levels of 1-companion model. The 2nd row shows the levels of 2-companion model. The 3rd row shows the levels of 3-companion model.}}{15}{figure.4}}
\newlabel{fig:levels-122}{{4}{15}{The 1st row shows the levels of 1-companion model. The 2nd row shows the levels of 2-companion model. The 3rd row shows the levels of 3-companion model}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The 1st row shows the fit of 1-companion model. The 2nd row shows the fit of 2-companion model. The 3rd row shows the fit of 3-companion model. All the fits are drawn from the posterior of each model. The companion model is clearly not as good as the other two. The 2-companion and 3-companion models are not distinguishable from this view.}}{16}{figure.5}}
\newlabel{fig:fit-122-1-2-3}{{5}{16}{The 1st row shows the fit of 1-companion model. The 2nd row shows the fit of 2-companion model. The 3rd row shows the fit of 3-companion model. All the fits are drawn from the posterior of each model. The companion model is clearly not as good as the other two. The 2-companion and 3-companion models are not distinguishable from this view}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The 1st row shows the fit of 2-companion model zoomed in. The 2nd row shows the fit of 3-companion model zoomed in. The \leavevmode {\color  {red}red curves} in the center indicate the optimal fits. The 3-companion fit is only slightly better than the 2-companion fit. Because the optimally-fit 'well' in the 3-companion fit is too shallow. Some of the 3-companion fits actually come from local minima. (In the figure, some fits are spiky which indicates over-fitting.)}}{17}{figure.6}}
\newlabel{fig:fit-122-2-3-zoom}{{6}{17}{The 1st row shows the fit of 2-companion model zoomed in. The 2nd row shows the fit of 3-companion model zoomed in. The \textcolor {red}{red curves} in the center indicate the optimal fits. The 3-companion fit is only slightly better than the 2-companion fit. Because the optimally-fit 'well' in the 3-companion fit is too shallow. Some of the 3-companion fits actually come from local minima. (In the figure, some fits are spiky which indicates over-fitting.)\relax }{figure.6}{}}
