\relax 
\ifx\hyper@anchor\@undefined
\global \let \oldcontentsline\contentsline
\gdef \contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global \let \oldnewlabel\newlabel
\gdef \newlabel#1#2{\newlabelxx{#1}#2}
\gdef \newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\let \contentsline\oldcontentsline
\let \newlabel\oldnewlabel}
\else
\global \let \hyper@last\relax 
\fi

\citation{brewer11a}
\citation{goodman10a,hou12a,foreman-mackey13a}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{eq:bayesian-decision-theory}{{1}{1}{Introduction\relax }{equation.1.1}{}}
\citation{skilling06a}
\@writefile{toc}{\contentsline {section}{\numberline {2}Diffusive Nested Sampling}{2}{section.2}}
\newlabel{eq:prior-mass}{{5}{2}{Diffusive Nested Sampling\relax }{equation.2.5}{}}
\newlabel{eq:dM}{{6}{2}{Diffusive Nested Sampling\relax }{equation.2.6}{}}
\newlabel{eq:int-dM}{{7}{2}{Diffusive Nested Sampling\relax }{equation.2.7}{}}
\newlabel{eq:evidence-prior-mass}{{8}{2}{Diffusive Nested Sampling\relax }{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Level and Constrained Prior}{2}{subsection.2.1}}
\newlabel{eq:constrained-prior}{{9}{2}{Level and Constrained Prior\relax }{equation.2.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Setting Level Thresholds}{3}{subsection.2.2}}
\newlabel{sec:constructing}{{2.2}{3}{Setting Level Thresholds\relax }{subsection.2.2}{}}
\citation{goodman10a}
\newlabel{eq:mixture-constrained-prior}{{10}{4}{Setting Level Thresholds\relax }{equation.2.10}{}}
\newlabel{eq:weight-sum-1}{{11}{4}{Setting Level Thresholds\relax }{equation.2.11}{}}
\newlabel{eq:importance-sampling}{{12}{4}{Setting Level Thresholds\relax }{equation.2.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Nested Sampling by Stretch Move}{4}{subsection.2.3}}
\newlabel{sec:algorithm}{{2.3}{4}{Nested Sampling by Stretch Move\relax }{subsection.2.3}{}}
\citation{goodman10a}
\citation{brewer11a}
\citation{brewer11a}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Refining Level Masses}{5}{subsection.2.4}}
\newlabel{sec:refining-level-masses}{{2.4}{5}{Refining Level Masses\relax }{subsection.2.4}{}}
\citation{brewer11a}
\newlabel{eq:refinement}{{13}{6}{Refining Level Masses\relax }{equation.2.13}{}}
\newlabel{eq:R}{{14}{6}{Refining Level Masses\relax }{}{}}
\newlabel{eq:refine-expectation}{{16}{6}{Refining Level Masses\relax }{equation.2.16}{}}
\newlabel{eq:refine-variance}{{17}{6}{Refining Level Masses\relax }{equation.2.16}{}}
\citation{brewer11a}
\newlabel{eq:refine-interval-expectation}{{18}{7}{Refining Level Masses\relax }{equation.2.18}{}}
\newlabel{eq:refine-interval-variance}{{19}{7}{Refining Level Masses\relax }{equation.2.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Computing Evidence}{7}{subsection.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}2-d Gaussian Testing Case}{8}{section.3}}
\newlabel{eq:likelihood2}{{23}{8}{2-d Gaussian Testing Case\relax }{equation.3.23}{}}
\newlabel{eq:prior2}{{24}{8}{2-d Gaussian Testing Case\relax }{equation.3.24}{}}
\newlabel{eq:analytical-threshold}{{26}{8}{2-d Gaussian Testing Case\relax }{equation.3.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Testing Level Thresholds Setting}{8}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Testing Constrained Prior Mixture}{9}{subsection.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {4}High-Dimension Gaussian Testing Case}{9}{section.4}}
\newlabel{eq:likelihood10}{{27}{9}{High-Dimension Gaussian Testing Case\relax }{equation.4.27}{}}
\newlabel{eq:prior10}{{28}{9}{High-Dimension Gaussian Testing Case\relax }{equation.4.28}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Testing Prior Mass Refinement}{9}{subsection.4.1}}
\bibcite{brewer11a}{{1}{2011}{{Brewer \emph  {et\tmspace  +\thinmuskip {.1667em}al.}}}{{}}}
\bibcite{foreman-mackey13a}{{2}{2013}{{Foreman-Mackey \emph  {et\tmspace  +\thinmuskip {.1667em}al.}}}{{}}}
\bibcite{goodman10a}{{3}{2010}{{Goodman \emph  {et\tmspace  +\thinmuskip {.1667em}al.}}}{{}}}
\bibcite{hou12a}{{4}{2012}{{Hou \emph  {et\tmspace  +\thinmuskip {.1667em}al.}}}{{}}}
\bibcite{skilling06a}{{5}{2006}{{Skilling }}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Exoplanet for Star 122}{10}{section.5}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces The optimal-fit parameters for all 3 models are summarized here. The 1st row lists the log likelihoods of the optimal-fit parameters of these 3 models. }}{11}{table.1}}
\newlabel{tab:best-fit-star-122}{{1}{11}{The optimal-fit parameters for all 3 models are summarized here. The 1st row lists the log likelihoods of the optimal-fit parameters of these 3 models. \relax }{table.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The experiment was repeated $10,000$ times. The experimental mean values of the logarithm of the 6 levels' thresholds in the table are the mean of the $10,000$ repetitions. Notice that the experimental means for $N_{1b}$ tend to be closer to analytical values than those for $N_{1a}$ as expected.}}{12}{table.2}}
\newlabel{tab:threshold-mean}{{2}{12}{The experiment was repeated $10,000$ times. The experimental mean values of the logarithm of the 6 levels' thresholds in the table are the mean of the $10,000$ repetitions. Notice that the experimental means for $N_{1b}$ tend to be closer to analytical values than those for $N_{1a}$ as expected}{table.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The experiment was repeated $10,000$ times. The experimental std of the logarithm of the 6 levels' thresholds in the table are the variance of the $10,000$ repetitions. The algorithm almost achieves the expected precision. Note that the std's for $N_{1a}$ are approximately $\sqrt  {10}$ times those for $N_{1b}$.}}{12}{table.3}}
\newlabel{tab:threshold-variance}{{3}{12}{The experiment was repeated $10,000$ times. The experimental std of the logarithm of the 6 levels' thresholds in the table are the variance of the $10,000$ repetitions. The algorithm almost achieves the expected precision. Note that the std's for $N_{1a}$ are approximately $\sqrt {10}$ times those for $N_{1b}$}{table.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces True levels' thresholds and prior masses are listed in the table. 10 Levels are given. We keep many digits because these are true levels.}}{13}{table.4}}
\newlabel{tab:true-levels}{{4}{13}{True levels' thresholds and prior masses are listed in the table. 10 Levels are given. We keep many digits because these are true levels}{table.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces The standard deviations for different $N_1$'s and $N_2$'s. The experiments are repeated for $1,000$ times. Compared with evidence value $9.77\times 10^{-14}$, all the standard deviations are reasonably small. But $N_2$ is clearly more important in reducing variance. Note that there are 30 levels in this case, $30\times N_1$ and $N_2$ are actually comparable.}}{13}{table.5}}
\newlabel{tab:N1N2}{{5}{13}{The standard deviations for different $N_1$'s and $N_2$'s. The experiments are repeated for $1,000$ times. Compared with evidence value $9.77\times 10^{-14}$, all the standard deviations are reasonably small. But $N_2$ is clearly more important in reducing variance. Note that there are 30 levels in this case, $30\times N_1$ and $N_2$ are actually comparable}{table.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Level 1 (upper) and Level 6 (lower): Both histograms are plotted with 25 bins and 10,000 samples. (Here samples mean repetitions of the experiment, not the samples of likelihood to build every single level.) The left-hand side is the histogram of levels built with $N_{1a}$ likelihoods and the right-hand side is the histogram of levels built with $N_{1b}$ likelihoods. The \leavevmode {\color  {red}red} solid lines indicate the true values of the logarithm of the likelihood thresholds of levels and \leavevmode {\color  {darkgreen}dark green} solid lines indicate the experimental mean of the logarithm of the likelihood thresholds, which cannot be distinguished from the true values in these pictures. The \leavevmode {\color  {red}red} dashed lines indicate the theoretical standard deviation and the \leavevmode {\color  {darkgreen}dark green} dashed lines indicate the experimental standard deviation.}}{14}{figure.1}}
\newlabel{fig:level1-6}{{1}{14}{Level 1 (upper) and Level 6 (lower): Both histograms are plotted with 25 bins and 10,000 samples. (Here samples mean repetitions of the experiment, not the samples of likelihood to build every single level.) The left-hand side is the histogram of levels built with $N_{1a}$ likelihoods and the right-hand side is the histogram of levels built with $N_{1b}$ likelihoods. The \textcolor {red}{red} solid lines indicate the true values of the logarithm of the likelihood thresholds of levels and \textcolor {darkgreen}{dark green} solid lines indicate the experimental mean of the logarithm of the likelihood thresholds, which cannot be distinguished from the true values in these pictures. The \textcolor {red}{red} dashed lines indicate the theoretical standard deviation and the \textcolor {darkgreen}{dark green} dashed lines indicate the experimental standard deviation}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Histograms of prior masses of samples inside a bin sandwiched by two adjacent levels. 4 examples are given. All are approximately uniform distribution.}}{15}{figure.2}}
\newlabel{fig:hist-gaps}{{2}{15}{Histograms of prior masses of samples inside a bin sandwiched by two adjacent levels. 4 examples are given. All are approximately uniform distribution}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces The 10 levels in the figure are the true levels summarized in Tab. (\ref  {tab:true-levels}). Notice that the standard deviation of likelihood samples between level 3 ($\qopname  \relax o{log}{M} = -3$) and level 6 ($\qopname  \relax o{log}{M} = -6$) will pretty much determine the standard deviation of the final result.}}{16}{figure.3}}
\newlabel{fig:level-var}{{3}{16}{The 10 levels in the figure are the true levels summarized in Tab. (\ref {tab:true-levels}). Notice that the standard deviation of likelihood samples between level 3 ($\log {M} = -3$) and level 6 ($\log {M} = -6$) will pretty much determine the standard deviation of the final result}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The 1st row shows the levels of 1-companion model. The 2nd row shows the levels of 2-companion model. The 3rd row shows the levels of 3-companion model.}}{17}{figure.4}}
\newlabel{fig:levels-122}{{4}{17}{The 1st row shows the levels of 1-companion model. The 2nd row shows the levels of 2-companion model. The 3rd row shows the levels of 3-companion model}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The 1st row shows the fit of 1-companion model. The 2nd row shows the fit of 2-companion model. The 3rd row shows the fit of 3-companion model. All the fits are drawn from the posterior of each model. The companion model is clearly not as good as the other two. The 2-companion and 3-companion models are not distinguishable from this view.}}{18}{figure.5}}
\newlabel{fig:fit-122-1-2-3}{{5}{18}{The 1st row shows the fit of 1-companion model. The 2nd row shows the fit of 2-companion model. The 3rd row shows the fit of 3-companion model. All the fits are drawn from the posterior of each model. The companion model is clearly not as good as the other two. The 2-companion and 3-companion models are not distinguishable from this view}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The 1st row shows the fit of 2-companion model zoomed in. The 2nd row shows the fit of 3-companion model zoomed in. The \leavevmode {\color  {red}red curves} in the center indicate the optimal fits. The 3-companion fit is only slightly better than the 2-companion fit. Because the optimally-fit 'well' in the 3-companion fit is too shallow. Some of the 3-companion fits actually come from local minima. (In the figure, some fits are spiky which indicates over-fitting.)}}{19}{figure.6}}
\newlabel{fig:fit-122-2-3-zoom}{{6}{19}{The 1st row shows the fit of 2-companion model zoomed in. The 2nd row shows the fit of 3-companion model zoomed in. The \textcolor {red}{red curves} in the center indicate the optimal fits. The 3-companion fit is only slightly better than the 2-companion fit. Because the optimally-fit 'well' in the 3-companion fit is too shallow. Some of the 3-companion fits actually come from local minima. (In the figure, some fits are spiky which indicates over-fitting.)\relax }{figure.6}{}}
